{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEYgViv5vM3A"
      },
      "outputs": [],
      "source": [
        "!pip install sidetable -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sidetable\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import holidays\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import (\n",
        "    MinMaxScaler\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    KFold,StratifiedKFold,cross_val_score, train_test_split\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    r2_score\n",
        ")\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from scipy import stats\n",
        "\n",
        "# Importing pipelines\n",
        "from sklearn.pipeline import  Pipeline\n",
        "from sklearn.compose import(\n",
        "    ColumnTransformer,\n",
        ")\n",
        "\n",
        "from sklearn.base import (\n",
        "    BaseEstimator, TransformerMixin\n",
        ")\n",
        "\n",
        "# Importing class, to load and save predictive models in external files.\n",
        "import pickle,joblib\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pktjqmAlvUPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropColumns(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.drop(labels=self.columns, axis=1)"
      ],
      "metadata": {
        "id": "nGMF44ZvzS1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decomposition(df, date_col, target_col, filial_col):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Garantir datetime e índice\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    df = df.set_index(date_col)\n",
        "\n",
        "    # Criar colunas de período\n",
        "    df['ano']    = df.index.to_period('Y')\n",
        "    df['mes']    = df.index.to_period('M')\n",
        "    df['semana'] = df.index.to_period('W')\n",
        "    df['dia']    = df.index.to_period('D')\n",
        "\n",
        "    # Totais por filial + período\n",
        "    df['qtd_per_year']  = df.groupby([filial_col, 'ano'])[target_col].transform('sum')\n",
        "    df['qtd_per_month'] = df.groupby([filial_col, 'mes'])[target_col].transform('sum')\n",
        "    df['qtd_per_week']  = df.groupby([filial_col, 'semana'])[target_col].transform('sum')\n",
        "    df['qtd_per_day']   = df.groupby([filial_col, 'dia'])[target_col].transform('sum')\n",
        "\n",
        "    # Médias por filial + período\n",
        "    df['avg_qtd_per_year']  = df.groupby([filial_col, 'ano'])[target_col].transform('mean')\n",
        "    df['avg_qtd_per_month'] = df.groupby([filial_col, 'mes'])[target_col].transform('mean')\n",
        "    df['avg_qtd_per_week']  = df.groupby([filial_col, 'semana'])[target_col].transform('mean')\n",
        "    df['avg_qtd_per_day']   = df.groupby([filial_col, 'dia'])[target_col].transform('mean')\n",
        "\n",
        "    # Restaurar índice original\n",
        "    return df.reset_index()\n"
      ],
      "metadata": {
        "id": "pQhFx2ADvbeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_holidays(df):\n",
        "\n",
        "    df['date']= pd.to_datetime(df.date)\n",
        "\n",
        "    min_year = df.date.min().year\n",
        "    max_year = df.date.max().year\n",
        "\n",
        "    years_list = pd.period_range(min_year, max_year, freq='Y')\n",
        "\n",
        "    list_of_holidays = []\n",
        "    for year in years_list:\n",
        "        list_of_holidays.append(holidays.BR(years=int(str(year))).keys())\n",
        "\n",
        "    holiday_list = [item for sublist in list_of_holidays for item in sublist]\n",
        "\n",
        "    return holiday_list"
      ],
      "metadata": {
        "id": "0GwLEfi_vgbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dt_attributes(df):\n",
        "    df['date'] = pd.to_datetime(df.date)\n",
        "    df['month'] = df.date.dt.month\n",
        "    df['day_of_month'] = df.date.dt.day\n",
        "    df['day_of_year'] = df.date.dt.dayofyear\n",
        "    df['week_of_year'] = df.date.dt.isocalendar().week\n",
        "    df['day_of_week'] = df.date.dt.weekday + 1\n",
        "    df['year'] = df.date.dt.year\n",
        "    df['is_weekend'] = df.date.dt.weekday // 5\n",
        "    df['start_of_month'] = df['day_of_month'].apply(lambda x: 1 if x <= 5 else 0)\n",
        "    df['end_of_month']   = df['day_of_month'].apply(lambda x: 1 if x >= 25 else 0)\n",
        "    df['is_holiday'] = np.where(df.date.isin(list_holidays(df)), 1, 0)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "JFnrpieivlZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_stationarity(serie):\n",
        "    # Calcula estatísticas móveis\n",
        "    rolmean = serie.rolling(window=12).mean()\n",
        "    rolstd = serie.rolling(window=12).std()\n",
        "\n",
        "    # Plot das estatísticas móveis\n",
        "    orig = plt.plot(serie, color='blue', label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Média Móvel')\n",
        "    std = plt.plot(rolstd, color='black', label='Desvio Padrão')\n",
        "\n",
        "    # Plot\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Estatísticas Móveis - Média e Desvio Padrão')\n",
        "    plt.show()\n",
        "\n",
        "    # Teste Dickey-Fuller:\n",
        "    # Print\n",
        "    print('\\nResultado do Teste Dickey-Fuller:\\n')\n",
        "\n",
        "    # Test\n",
        "    dfteste = adfuller(serie, autolag='AIC')\n",
        "\n",
        "    # Formatting the output\n",
        "    dfsaida = pd.Series(dfteste[0:4], index=['Estatística do Teste',\n",
        "                                             'Valor-p',\n",
        "                                             'Número de Lags Consideradas',\n",
        "                                             'Número de Observações Usadas'])\n",
        "\n",
        "    # Loop por cada item da saída do teste\n",
        "    for key, value in dfteste[4].items():\n",
        "        dfsaida['Valor Crítico (%s)' % key] = value\n",
        "\n",
        "    # Print\n",
        "    print(dfsaida)\n",
        "\n",
        "    # Testa o valor-p\n",
        "    print('\\nConclusão:')\n",
        "    if dfsaida[1] > 0.05:\n",
        "        print('\\nO valor-p é maior que 0.05 e, portanto, não temos evidências para rejeitar a hipótese nula.')\n",
        "        print('Essa série provavelmente não é estacionária.')\n",
        "    else:\n",
        "        print('\\nO valor-p é menor que 0.05 e, portanto, temos evidências para rejeitar a hipótese nula.')\n",
        "        print('Essa série provavelmente é estacionária.')"
      ],
      "metadata": {
        "id": "ENdNFNXqvq4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to select the best features\n",
        "def feature_imp(features, target,param_imp,n_best_features):\n",
        "    # Define o classificador, ou seja, instância um objeto da classe XGBRegressor\n",
        "    reg_XBGR = xgb.XGBRFRegressor(verbosity=0, silent=True)\n",
        "\n",
        "    # ajuste os dados\n",
        "    reg_XBGR.fit(features, target)\n",
        "\n",
        "    # selecionando os melhores parâmetros com grid search, que indicar a importância relativa de cada atributo para fazer previsões precisas:\n",
        "    reg_XBGR_feature_imp = reg_XBGR.get_booster().get_score(importance_type=param_imp)\n",
        "\n",
        "    # obtém nome das colunas\n",
        "    keys = list(reg_XBGR_feature_imp.keys())\n",
        "\n",
        "    # obtém scores das features\n",
        "    values = list(reg_XBGR_feature_imp.values())\n",
        "\n",
        "    # crianndo dataframe  com  k recusros principais\n",
        "    xbg_best_features = pd.DataFrame(data=values, index=keys, columns=[\"score_XGBRFRegressor\"]).sort_values(\n",
        "        by=\"score_XGBRFRegressor\", ascending=True).nlargest(n_best_features, columns=\"score_XGBRFRegressor\")\n",
        "\n",
        "    # Return the best features\n",
        "    return xbg_best_features"
      ],
      "metadata": {
        "id": "KpZv-fDivvY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xgb_model_helper(X_train, y_train, PARAMETERS, V_PARAM_NAME=False, V_PARAM_VALUES=False, BR=10,):\n",
        "    # Cria uma matrix temporária em formato de bit do conjunto de dados a ser treinados\n",
        "    temp_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
        "\n",
        "    # Check os parâmetros a ser utilizados\n",
        "    if V_PARAM_VALUES == False:\n",
        "        cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5, num_boost_round=BR, params=PARAMETERS, as_pandas=True,\n",
        "                            seed=123)\n",
        "        return cv_results\n",
        "\n",
        "    else:\n",
        "        # Criando uma Lista, para armazenar os resultados e os nomes, de cada uma das métricas.\n",
        "        results = []\n",
        "\n",
        "        # Percorre a lista de parâmetros\n",
        "        for v_param_value in V_PARAM_VALUES:\n",
        "            # Adicionando o nome dos parâmetros avaliado a lista de nomes.\n",
        "            PARAMETERS[V_PARAM_NAME] = v_param_value\n",
        "\n",
        "            # Treinando o modelo com Cross Validation.\n",
        "            cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5, num_boost_round=BR, params=PARAMETERS, as_pandas=True,\n",
        "                                seed=123)\n",
        "\n",
        "            # Adicionando os resultados gerados a lista de resultados.\n",
        "            results.append((cv_results[\"train-mae-mean\"].tail().values[-1], cv_results[\"test-mae-mean\"].tail().values[\n",
        "                -1]))  # .tail().values[-1] captura somente as colunas\n",
        "\n",
        "        # zip “pareia” os elementos de uma série de listas, tuplas ou outras sequências para criar uma lista de tuplas:\n",
        "\n",
        "        # Adicionando a média da AUC e o desvio-padrão dos resultados gerados, pelo modelo analisado ao Dataframe de médias.\n",
        "        data = list(zip(V_PARAM_VALUES, results))\n",
        "        print(pd.DataFrame(data, columns=[V_PARAM_NAME, \"mae\"]))\n",
        "\n",
        "        return cv_results"
      ],
      "metadata": {
        "id": "Vpl4QOGcv0WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def opt_number_of_boosting_rounds(X_train, y_train,):\n",
        "    # create the DMatrix\n",
        "    temp_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
        "\n",
        "    # Create the parameter dictionary for each tree: params\n",
        "    params = {\"objective\": 'reg:linear', \"max_depth\": 5}\n",
        "\n",
        "    # Create lis of number of boosting rounds\n",
        "    num_rounds = [5, 10, 20, 25, 50, 100]\n",
        "\n",
        "    # Empty list to store final round rmse per XGBoost model\n",
        "    final_rmse_per_round = []\n",
        "\n",
        "    # Iterate ove num_rounds and build one model per num_boost_round parameter\n",
        "    for curr_num_rounds in num_rounds:\n",
        "        # Perform cross-validation: cv_results\n",
        "        cv_results = xgb.cv(dtrain=temp_dmatrix, params=params, nfold=5, num_boost_round=curr_num_rounds,\n",
        "                            metrics=\"mae\", as_pandas=True, seed=123)\n",
        "        # Append final round RMSE\n",
        "        final_rmse_per_round.append(cv_results[\"test-mae-mean\"].tail().values[-1])\n",
        "    # print the resultant Dataframe\n",
        "    num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
        "\n",
        "    return pd.DataFrame(num_rounds_rmses, columns=[\"num_boosting_rounds\", \"mae\"])"
      ],
      "metadata": {
        "id": "BZxveh-cv4om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regression(model,x_test,y_test):\n",
        "\n",
        "    # Evaluating the model\n",
        "    y_pred  = model.predict(x_test)\n",
        "    print(\"Metrics in Test data:\\n \")\n",
        "    print('='*30)\n",
        "    # Calculate the error\n",
        "    print('MAE:', mean_absolute_error(y_test,y_pred))\n",
        "    print('MAPE:',mean_absolute_percentage_error(y_test,y_pred))\n",
        "    print('MSE:', mean_squared_error(y_test,y_pred))\n",
        "    #print('RMSE:',mean_squared_error(y_test,y_pred, squared = False))\n",
        "    print('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "    print('R2:',  r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9DNo1YNLv8_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_preditction(y_test,y_pred,title,template):\n",
        "    HEIGHT = 700\n",
        "    WIDTH = 950\n",
        "    TITLE_FONT={'size':20, 'family': 'Times New Roman',}\n",
        "    TITLE_X=0.5\n",
        "    FONT_COLOR = \"#000000\"\n",
        "    # Create the figures\n",
        "    fig = go.Figure([\n",
        "        #go.Scatter(y=y_train, name='train', mode='markers'),\n",
        "        go.Scatter(y=y_test, name='test', mode='markers'),\n",
        "        go.Scatter(y=y_pred, name='prediction')\n",
        "    ])\n",
        "\n",
        "    fig.update_layout(title_text=title,\n",
        "                      title_x=TITLE_X,\n",
        "                      title_font=TITLE_FONT,\n",
        "                      font_color=FONT_COLOR,\n",
        "                      height= HEIGHT,\n",
        "                      width= WIDTH,\n",
        "                      xaxis_title='Qtd. per Week',\n",
        "                      yaxis_title='Forecast',\n",
        "                      template= template,\n",
        "                     )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "ldkp6_a9v_AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/dataset_25.xlsx')"
      ],
      "metadata": {
        "id": "RIdQFQzOwDDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "PcaT2yn2x1H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "ssKsYh4Z8wmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.lower()"
      ],
      "metadata": {
        "id": "or2tu2EPxoXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.groupby(['date','filial'])['vol'].sum().reset_index()"
      ],
      "metadata": {
        "id": "0ltkdUuO35jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create copy dataframe\n",
        "df_cp = df.copy()\n",
        "\n",
        "# Remove todas as colunas indesejadas\n",
        "unwanted_columns = ['familia','class_prod','categ_prod','tipo_grupo', 'tipo_mfm', 'código tipo manfrim', '1']\n",
        "df.drop(unwanted_columns, axis = 1, inplace = True, errors = 'ignore')"
      ],
      "metadata": {
        "id": "iWyCXSw9yLCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combination of attributes\n",
        "df['vol'] = df['vol'] / 15\n",
        "# ou, se quiser renomear a coluna depois\n",
        "df.rename(columns={'vol': 'qtd_vendida'}, inplace=True)\n",
        "\n",
        "# Ordem cronológica da série\n",
        "df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# View combination\n",
        "df.head()"
      ],
      "metadata": {
        "id": "J9JialEHyeGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8Y60ZJNO_Meq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificando se existe Datas Faltantes\n",
        "\n",
        "# Gerando todas as datas do intervalo esperado\n",
        "datas_completas = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')\n",
        "\n",
        "# Verificando quais datas estão faltando\n",
        "datas_faltando  = datas_completas.difference(df['date'])\n",
        "\n",
        "datas_faltando"
      ],
      "metadata": {
        "id": "TZK6rBN3zt_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chaves = ['filial']\n",
        "\n",
        "# Datas completas\n",
        "df_datas = pd.DataFrame({'date': pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')})\n",
        "\n",
        "# Combinações únicas de chaves\n",
        "chaves_unicas = df[chaves].drop_duplicates()\n",
        "\n",
        "# Produto cartesiano\n",
        "df_base = chaves_unicas.merge(df_datas, how='cross')\n",
        "\n",
        "# Merge com dados reais\n",
        "df = df_base.merge(df[['date'] + chaves + ['qtd_vendida']], on=['date'] + chaves, how='left')\n",
        "\n",
        "# Preenchimento\n",
        "df['qtd_vendida'] = df['qtd_vendida'].fillna(0)"
      ],
      "metadata": {
        "id": "s0mqOQRn0RXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Ln6FEFkt_mxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_agg = (\n",
        "#    df\n",
        "#    .groupby([\n",
        "#        pd.Grouper(key='date', freq='W'),  # agrupa por semana\n",
        "#        'filial'\n",
        "#    ])['qtd_vendida']\n",
        "#    .sum()\n",
        "#    .reset_index()\n",
        "#    .sort_values(['filial', 'date'])\n",
        "#)"
      ],
      "metadata": {
        "id": "fQ28wK4TfcZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function call, time series decomposition\n",
        "# ts_decomp= decomposition(df_agg,'date','filial','qtd_vendida')\n",
        "ts_decomp = decomposition(df, date_col='date', target_col='qtd_vendida', filial_col='filial')\n",
        "\n",
        "\n",
        "# View decomposition\n",
        "ts_decomp.head()"
      ],
      "metadata": {
        "id": "HeUAFIA-1c-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicate records.\n",
        "print(f'Values duplicated: {ts_decomp.duplicated().sum()}')"
      ],
      "metadata": {
        "id": "CwJ4nt3M18iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_decomp.info()"
      ],
      "metadata": {
        "id": "D5el-SH_AKn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropna = ts_decomp.copy()"
      ],
      "metadata": {
        "id": "PsEC_37-B2fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates\n",
        "df_drop_dup=ts_decomp.drop_duplicates(subset=['date'],keep='last')\n"
      ],
      "metadata": {
        "id": "eOe_1MjA2LKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop_dup.stb.missing(clip_0=True, style=True)\n"
      ],
      "metadata": {
        "id": "9i942SEE2YTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop on column axis with nan values\n",
        "df_dropna=df_drop_dup.dropna(subset=['qtd_per_week'])\n",
        "\n",
        "# Analyzing if there are missing values in the dataset\n",
        "df_dropna.stb.missing(style=True)"
      ],
      "metadata": {
        "id": "9ovcq1E92xFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas indesejadas\n",
        "unwanted_columns = ['avg_qtd_per_year','qtd_per_year','qtd_per_month','avg_qtd_per_month', 'semana', 'ano','mes','dia', 'qtd_vendida']\n",
        "\n",
        "df_dropna.drop(unwanted_columns, axis = 1, inplace = True, errors = 'ignore')\n",
        "\n",
        "# Checking for duplicate records.\n",
        "print(f'Values duplicated: {df_dropna.duplicated().sum()}')"
      ],
      "metadata": {
        "id": "LAvLJSnX3x05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropna.info()"
      ],
      "metadata": {
        "id": "ACGWEmX02N1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df_dropna.groupby('filial').tail(3)\n"
      ],
      "metadata": {
        "id": "chfOGF40j0JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preparar_lags_semanais(df):\n",
        "    df = df.copy()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # Cria colunas auxiliares apenas para o agrupamento\n",
        "    df['ano'] = df['date'].dt.isocalendar().year\n",
        "    df['semana'] = df['date'].dt.isocalendar().week\n",
        "    df['inicio_semana'] = df['date'] - pd.to_timedelta(df['date'].dt.weekday, unit='D')\n",
        "\n",
        "    # Agrega por semana e filial\n",
        "    semanal = (\n",
        "        df.groupby(['filial', 'ano', 'semana'], as_index=False)\n",
        "          .agg({\n",
        "              'qtd_per_day': 'sum',\n",
        "              'inicio_semana': 'first'\n",
        "          })\n",
        "          .rename(columns={'qtd_per_day': 'qtd_per_week', 'inicio_semana': 'date'})\n",
        "    )\n",
        "\n",
        "    # Ordena corretamente por semana\n",
        "    semanal = semanal.sort_values(['filial', 'date'])\n",
        "\n",
        "    # Calcula lags e médias móveis defasadas\n",
        "    semanal['lag_1'] = semanal.groupby('filial')['qtd_per_week'].shift(1)\n",
        "    semanal['lag_2'] = semanal.groupby('filial')['qtd_per_week'].shift(2)\n",
        "    semanal['roll_mean_3'] = (\n",
        "        semanal.groupby('filial')['qtd_per_week']\n",
        "        .shift(1)\n",
        "        .rolling(3)\n",
        "        .mean()\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    semanal['roll_mean_4'] = (\n",
        "        semanal.groupby('filial')['qtd_per_week']\n",
        "        .shift(1)\n",
        "        .rolling(4)\n",
        "        .mean()\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Remove colunas auxiliares (ano, semana)\n",
        "    semanal = semanal.drop(columns=['ano', 'semana'])\n",
        "\n",
        "    return semanal\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yS1n7vbT4rX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = preparar_lags_semanais(df_dropna)"
      ],
      "metadata": {
        "id": "kPWMvHRfzfR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.groupby('filial').tail(5)"
      ],
      "metadata": {
        "id": "SJVgiQ7Lm6jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function SMA and MSD\n",
        "def sma_and_msd(df):\n",
        "\n",
        "    # Copy of dataframe\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Cria a variável com o retorno (mudança percentual do total sales per week)\n",
        "    # This will our variable target\n",
        "    df_copy[\"perc_return\"] = df_copy[\"qtd_per_week\"].pct_change(1) * 100\n",
        "\n",
        "    # Shift das colunas op sales toys\n",
        "    df_copy[\"tsw\"] = df_copy[\"qtd_per_week\"].shift(1)\n",
        "\n",
        "    # Calculando os OKR's\n",
        "\n",
        "    # Simple Moving Average (SMA)\n",
        "    df_copy[\"SMA_3\"] = df_copy[[\"qtd_per_week\"]].rolling(3).mean().shift(1)\n",
        "    df_copy[\"SMA_5\"] = df_copy[[\"qtd_per_week\"]].rolling(5).mean().shift(1)\n",
        "\n",
        "    # Moving Standard Deviation (MSD) - Volatilidade\n",
        "    df_copy[\"MSD_3\"] = df_copy[\"perc_return\"].rolling(3).std().shift(1)\n",
        "    df_copy[\"MSD_5\"] = df_copy[\"perc_return\"].rolling(5).std().shift(1)\n",
        "\n",
        "    return df_copy.dropna()\n",
        "\n",
        "# Function call\n",
        "df_new_01 = sma_and_msd(df_new)\n"
      ],
      "metadata": {
        "id": "MQtlYGz64Pd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ts = create_dt_attributes(df_new_01)\n",
        "df_ts.head()"
      ],
      "metadata": {
        "id": "zoDAlCyA5A-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ts.info()"
      ],
      "metadata": {
        "id": "NbgVGIuNCQ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte variáveis categóricas em variáveis dummy aplicando One-Hot Encoding\n",
        "df_dummies = pd.get_dummies(df_ts, columns = [ 'month','year', 'filial'])\n",
        "\n",
        "\n",
        "# Checking for duplicate records.\n",
        "print(f'\\nValues duplicated: {df_dummies.duplicated().sum()}')\n",
        "\n"
      ],
      "metadata": {
        "id": "OWZXbbs75Tm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_regular = df_dummies[(df_dummies['date'] >='2025-05-31')].index\n",
        "df_dummies.drop(ts_regular, inplace=True)\n"
      ],
      "metadata": {
        "id": "cnx0LyMw5zIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dummies.info()"
      ],
      "metadata": {
        "id": "B1EYQSw49s-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total = df_dummies.groupby('date').agg({'qtd_per_week': 'sum'}).sort_index()\n",
        "df_total = df_total.asfreq('W-MON')  # fixar frequência semanal\n",
        "df_total['qtd_per_week'] = df_total['qtd_per_week'].fillna(0)\n",
        "\n",
        "# Decomposição\n",
        "addi = sm.tsa.seasonal_decompose(df_total['qtd_per_week'], model='additive', extrapolate_trend='freq', period=7)\n",
        "mult = sm.tsa.seasonal_decompose(df_total['qtd_per_week'], model='multiplicative', extrapolate_trend='freq', period=7)\n",
        "\n",
        "# Plot\n",
        "plt.rcParams.update({'figure.figsize': (16,10)})\n",
        "addi.plot().suptitle('Additive Decomposition – Total Filiais', fontsize=22)\n",
        "mult.plot().suptitle('Multiplicative Decomposition – Total Filiais', fontsize=22)\n",
        "plt.show()\n",
        "\n",
        "# Teste ADF\n",
        "test_stationarity(df_total['qtd_per_week'])\n"
      ],
      "metadata": {
        "id": "R1HyUlnB6AZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas sem informações úteis ou com informações já derivadas serão descartadas.\n",
        "cols = [col for col in df_dummies.columns if col not in [\"date\", \"year\", \"month\",\"day_of_week\",\"qtd_per_week\",]]\n",
        "\n",
        "\n",
        "\n",
        "xFeatures, y = df_dummies[cols], df_dummies['qtd_per_week']\n"
      ],
      "metadata": {
        "id": "jpsf0zLi8q0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xFeatures.info()"
      ],
      "metadata": {
        "id": "qxbLCiMq6FxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xFeatures.head()"
      ],
      "metadata": {
        "id": "uabbjOzFEbKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function call Feature Importance\n",
        "xbg_best_features = feature_imp(features=xFeatures,\n",
        "                      target=y,\n",
        "                      param_imp='weight',\n",
        "                      n_best_features=7)\n",
        "\n",
        "# Capturing k best features for model training\n",
        "xbg_best_features"
      ],
      "metadata": {
        "id": "pw1ZNga79C6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extras = [\n",
        "    'start_of_month',\n",
        "    'end_of_month',\n",
        "    'year_2023',\n",
        "    'year_2024',\n",
        "    'year_2025'\n",
        "]\n",
        "# Variáveis selecionadas automaticamente\n",
        "auto_features = xbg_best_features.index.tolist()\n",
        "\n",
        "# Junta e remove duplicatas\n",
        "final_features = list(set(auto_features + extras))\n",
        "\n",
        "final_features = ['year_2024',\n",
        " 'start_of_month',\n",
        " 'year_2025',\n",
        " 'end_of_month',\n",
        " 'lag_2',\n",
        " 'day_of_year',\n",
        " 'lag_1',\n",
        " 'year_2023',\n",
        " 'day_of_month',\n",
        " 'roll_mean_3',\n",
        " 'roll_mean_4',\n",
        " 'filial_1',\n",
        " 'filial_3',\n",
        " 'filial_4',\n",
        " 'filial_5',\n",
        " 'filial_7',\n",
        " 'roll_mean_3',\n",
        " 'roll_mean_4',\n",
        "  'month_1',\n",
        "  'month_2',\n",
        "  'month_3',\n",
        "  'month_4',\n",
        "  'month_5',\n",
        "  'month_6',\n",
        "  'month_7',\n",
        "  'month_8',\n",
        "  'month_9',\n",
        "  'month_10',\n",
        "  'month_11',\n",
        "  'month_12'\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "zVDO--TRMMZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separando variáveis em componentes de input e output\n",
        "\n",
        "# semente do gerador de números aleatórios\n",
        "SEED = 42\n",
        "\n",
        "# Separaremos 20% dos dados para testes\n",
        "TEST_SIZE= 0.2\n",
        "\n",
        "# Creating new dataframe with top k features\n",
        "#X = xFeatures.loc[:,xbg_best_features.index]\n",
        "X = xFeatures[final_features]\n",
        "\n",
        "\n",
        "# divisão dos dados train/test\n",
        "X_train, X_test, y_train, y_test =train_test_split(X,y,random_state=SEED,test_size=TEST_SIZE)\n",
        "\n",
        "# shape\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "r5qmS0Ab9Ri0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Padronizador\n",
        "scaled_train = MinMaxScaler()\n",
        "\n",
        "# Aplicação da normalização utilizando a Function MinMaxScaler(), ou seja, transforme os dados\n",
        "X_train_min_max = scaled_train.fit_transform(X_train)\n",
        "\n",
        "# Aplicamos o fit somente nos dados de treino e aplicamos o transform nos dados de teste\n",
        "X_test_min_max = scaled_train.transform(X_test)\n",
        "\n",
        "# Padronizador da variável target\n",
        "scaler_target = MinMaxScaler()\n",
        "\n",
        "# Aplicação da normalização utilizando a Function MinMaxScaler(), ou seja, transforme os dados\n",
        "y_train_min_max = scaler_target.fit_transform(y_train.to_numpy().reshape(-1,1))\n",
        "\n",
        "# Aplicamos o fit somente nos dados de treino e aplicamos o transform nos dados de teste\n",
        "y_test_min_max = scaler_target.transform(y_test.to_numpy().reshape(-1,1))"
      ],
      "metadata": {
        "id": "I7dsKra49ZvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the desired parameters\n",
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"booster\":'gblinear',}\n",
        "\n",
        "# Function call\n",
        "xgb_model_helper(X_train_min_max,y_train,PARAMETERS)"
      ],
      "metadata": {
        "id": "UtDFE3fM9osH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5,\n",
        "}\n",
        "xgb_model_helper(X_train_min_max,y_train,PARAMETERS)"
      ],
      "metadata": {
        "id": "XaLFgU8x9xNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Qtd: {np.mean(y):,.2f}')"
      ],
      "metadata": {
        "id": "PYT7GUUT9-4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_number_of_boosting_rounds(X_train_min_max,y_train)\n"
      ],
      "metadata": {
        "id": "-3rQT1zq-sjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5, # Control Overfitting\n",
        "}\n",
        "V_PARAM_NAME=\"max_depth\"\n",
        "V_PARAM_VALUES=range(3,11,1)\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "c3ueA1Bu_JEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5,\n",
        "            \"max_depth\":4,\n",
        "\n",
        "}\n",
        "V_PARAM_NAME=\"gamma\"\n",
        "V_PARAM_VALUES = np.linspace(start=1e-2, stop=2, num=20).tolist()\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "-bmwEsdi_c3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5,\n",
        "            \"max_depth\":4,\n",
        "            \"gamma\":0.42,\n",
        "}\n",
        "V_PARAM_NAME=\"subsample\"\n",
        "V_PARAM_VALUES = [.5,.6,.7,.75,.8,.85,.9,]\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "46B-2BpP_0tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5,\n",
        "            \"max_depth\":4,\n",
        "            \"gamma\":0.42,\n",
        "            \"subsample\":0.90,\n",
        "}\n",
        "V_PARAM_NAME = \"colsample_bytree\"\n",
        "V_PARAM_VALUES = [.5,.6,.7,.75,.8,.85,.9,]\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "ny4DdQsAAROV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5,\n",
        "            \"max_depth\":4,\n",
        "            \"gamma\":0.42,\n",
        "            \"subsample\":0.9,\n",
        "            \"colsample_bytree\":0.75\n",
        "}\n",
        "V_PARAM_NAME = \"reg_alpha\"\n",
        "V_PARAM_VALUES = np.linspace(start=1e-3, stop=2, num=40).tolist()\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "UK7vf3XeApZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"learning_rate\": 0.5,\n",
        "            \"max_depth\":4,\n",
        "            \"gamma\":0.42,\n",
        "            \"subsample\":0.90,\n",
        "            \"colsample_bytree\":0.75,\n",
        "            \"reg_alpha\":2\n",
        "}\n",
        "V_PARAM_NAME = \"reg_lambda\"\n",
        "V_PARAM_VALUES = np.linspace(start=1e-3, stop=2, num=40).tolist()\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "rmLV53j4A_SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"max_depth\":4,\n",
        "            \"gamma\":0.42,\n",
        "            \"subsample\":0.90,\n",
        "            \"colsample_bytree\":0.75,\n",
        "            \"reg_alpha\":2,\n",
        "            \"reg_lambda\": 1e-3\n",
        "}\n",
        "V_PARAM_NAME = \"learning_rate\"\n",
        "V_PARAM_VALUES = np.linspace(start=1e-3, stop=0.5, num=40).tolist()\n",
        "\n",
        "data=xgb_model_helper(X_train_min_max,y_train,PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES,BR=25)"
      ],
      "metadata": {
        "id": "mSR_R_ksCDdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAMETERS={\"objective\":'reg:linear',\n",
        "            \"booster\":'gblinear',\n",
        "            \"eval_metric\":\"mae\",\n",
        "            \"max_depth\":4,\n",
        "            \"gamma\":0.42,\n",
        "            \"subsample\":0.90,\n",
        "            \"colsample_bytree\":0.75,\n",
        "            \"reg_alpha\":2,\n",
        "            \"reg_lambda\": 1e-3,\n",
        "            \"learning_rate\":0.5\n",
        "}\n",
        "# Creating an instance of the XGBRegressor model class\n",
        "reg_XGB = xgb.XGBRegressor(objective=\"reg:linear\",\n",
        "                           booster=\"gblinear\",\n",
        "                           eval_metric=\"mae\",\n",
        "                           max_depth=4,\n",
        "                           subsample=0.9,\n",
        "                           colsample_bytree=0.75,\n",
        "                           reg_alpha=2,\n",
        "                           reg_lambda=1e-3,\n",
        "                           learning_rate=0.5,\n",
        "                           gamma=0.42,\n",
        "                           num_boost_round=25,\n",
        "                           n_estimators=500,\n",
        ")\n",
        "# Fit the data (train) the model\n",
        "reg_XGB.fit(X_train_min_max, y_train)\n",
        "\n",
        "# Model Score, Evaluating Better Using Cross Validation\n",
        "reg_xgb_rmse = -cross_val_score(reg_XGB, X_train_min_max, y_train, cv =10,scoring=\"neg_root_mean_squared_error\")\n",
        "\n",
        "# Printing the result\n",
        "print('Summary')\n",
        "print('='*20)\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "pd.Series(reg_xgb_rmse).describe()"
      ],
      "metadata": {
        "id": "3NFxL9sECk-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_XGB.set_params\n"
      ],
      "metadata": {
        "id": "AwzzI-0JDOD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "evaluate=regression(reg_XGB, X_test_min_max, y_test)"
      ],
      "metadata": {
        "id": "WbhbghR7DXy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an the new dataframe with predictions\n",
        "df_predict, df_predict['predictions'] = pd.DataFrame(y_test), reg_XGB.predict(X_test_min_max)\n",
        "df_predict = df_predict.sort_index(ascending=True)\n",
        "\n",
        "# View predict\n",
        "df_predict"
      ],
      "metadata": {
        "id": "ozun15R0D8J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_predict['predictions'] = df_predict['predictions'].clip(lower=0)\n"
      ],
      "metadata": {
        "id": "S2W5IOBAZnPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_preditction(y_test=df_predict.qtd_per_week,\n",
        "                 y_pred=df_predict.predictions,\n",
        "                 title='<b>Qtd. Forecast by Week</b>',\n",
        "                 template='xgridoff'\n",
        "                )"
      ],
      "metadata": {
        "id": "dwEC-upOEUHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate confidence interval for the RMSE\n",
        "def ci_rmse(y_pred, y_test, confidence = .95):\n",
        "\n",
        "    # Capture quadratic errors\n",
        "    squared_errors = (y_pred - y_test) ** 2\n",
        "\n",
        "    # Return CI\n",
        "    return np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
        "                         loc=squared_errors.mean(),scale=stats.sem(squared_errors)))\n",
        "\n",
        "# Function call\n",
        "ci_rmse(reg_XGB.predict(X_test_min_max), y_test)"
      ],
      "metadata": {
        "id": "llBBNtp6FOID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coeficientes\n",
        "df_coef= pd.DataFrame(reg_XGB.coef_, X.columns, columns = ['coefficient']).sort_values('coefficient',ascending= False)\n",
        "\n",
        "# set colors\n",
        "colors = ['Positive' if c > 0 else 'Negative' for c in df_coef.coefficient]\n",
        "\n",
        "fig = px.bar(\n",
        "    x=df_coef.index,y=df_coef.coefficient, color=colors,\n",
        "    color_discrete_sequence=['green', 'red'],\n",
        "    labels=dict(x='Features', y='Linear coefficient'),\n",
        "    title='<b>Weight of each resource for forecasting total sales per week</b>'\n",
        ")\n",
        "\n",
        "fig.update_layout(title_font={'size':20, 'family': 'Times New Roman',},\n",
        "                  font_color=\"#000000\",\n",
        "                  height=700,\n",
        "                  template= \"xgridoff\",)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0-d2KG6TrkcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "challenge_columns = df_cp.columns\n",
        "\n",
        "features = X.columns.tolist()\n",
        "target = ['qtd_per_week']\n",
        "\n",
        "# Remove todas as colunas indesejadas\n",
        "unwanted_columns = list((set(challenge_columns) - set(target)) - set(features))\n",
        "unwanted_columns\n"
      ],
      "metadata": {
        "id": "FbnLwR_cuqGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma instância do transformador, passando como parâmetro as colunas que não queremos\n",
        "unwanted_columns = [col for col in unwanted_columns if col in X_train.columns]\n",
        "\n",
        "\n",
        "drop_columns = DropColumns(unwanted_columns)\n",
        "\n",
        "\n",
        "#Criando um pipeline completo com preparação e previsão\n",
        "full_pipeline_with_predictor = Pipeline([\n",
        "                        ('drop_cols',drop_columns),\n",
        "                         ('scaler',MinMaxScaler()),\n",
        "                         ('reg_XGB',reg_XGB)\n",
        "])\n",
        "full_pipeline_with_predictor\n",
        "\n",
        "# Ajusta o pipeline aos dados de treino\n",
        "full_pipeline_with_predictor.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XJLZrDpvuzxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(full_pipeline_with_predictor , \"/content/model/ts_total_sales_per_week.pkl\")\n"
      ],
      "metadata": {
        "id": "yRtkXtkuyTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model\n",
        "model_loaded = joblib.load(\"/content/model/ts_total_sales_per_week.pkl\")\n",
        "model_loaded"
      ],
      "metadata": {
        "id": "4sVfQFoQzrcR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}